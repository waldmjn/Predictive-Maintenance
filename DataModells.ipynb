{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0    M                298.1                    308.6                    1551   \n",
       "1    L                298.2                    308.7                    1408   \n",
       "2    L                298.1                    308.5                    1498   \n",
       "3    L                298.2                    308.6                    1433   \n",
       "4    L                298.2                    308.7                    1408   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
       "0         42.8                0       0   No Failure  \n",
       "1         46.3                3       0   No Failure  \n",
       "2         49.4                5       0   No Failure  \n",
       "3         39.5                7       0   No Failure  \n",
       "4         40.0                9       0   No Failure  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import TomekLinks, NearMiss, ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, BorderlineSMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filepath = \"C:/Users/WALDMJN/OneDrive - Schaeffler/Uni/Data Exploration Project/Pred Maintenance Project/Predictive-Maintenance/Data/predictive_maintenance.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df = df.drop([\"UDI\", \"Product ID\"], axis = 1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop out the target anomalies from notebook before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9991"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_df = df[df['Target'] == 1]\n",
    "indexPossibleFailure = fail_df[fail_df['Failure Type'] == 'No Failure'].index\n",
    "df.drop(indexPossibleFailure, axis=0, inplace=True)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9973"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_df  = df[df['Target'] == 0]\n",
    "indexPossibleFailure = fail_df[fail_df['Failure Type'] == 'Random Failures'].index\n",
    "df.drop(indexPossibleFailure, axis=0, inplace=True)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "      <th>Power [W]</th>\n",
       "      <th>Overstrain [minNm]</th>\n",
       "      <th>Heat dissipation [rpminK]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "      <td>6951.590560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16285.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "      <td>6826.722724</td>\n",
       "      <td>138.9</td>\n",
       "      <td>14784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "      <td>7749.387543</td>\n",
       "      <td>247.0</td>\n",
       "      <td>15579.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "      <td>5927.504659</td>\n",
       "      <td>276.5</td>\n",
       "      <td>14903.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "      <td>5897.816608</td>\n",
       "      <td>360.0</td>\n",
       "      <td>14784.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0    M                298.1                    308.6                    1551   \n",
       "1    L                298.2                    308.7                    1408   \n",
       "2    L                298.1                    308.5                    1498   \n",
       "3    L                298.2                    308.6                    1433   \n",
       "4    L                298.2                    308.7                    1408   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min]  Target Failure Type    Power [W]  \\\n",
       "0         42.8                0       0   No Failure  6951.590560   \n",
       "1         46.3                3       0   No Failure  6826.722724   \n",
       "2         49.4                5       0   No Failure  7749.387543   \n",
       "3         39.5                7       0   No Failure  5927.504659   \n",
       "4         40.0                9       0   No Failure  5897.816608   \n",
       "\n",
       "   Overstrain [minNm]  Heat dissipation [rpminK]  \n",
       "0                 0.0                    16285.5  \n",
       "1               138.9                    14784.0  \n",
       "2               247.0                    15579.2  \n",
       "3               276.5                    14903.2  \n",
       "4               360.0                    14784.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Power [W]'] = df['Torque [Nm]'] * (2 * np.pi * df['Rotational speed [rpm]'] / 60.0)\n",
    "df['Overstrain [minNm]'] = df['Torque [Nm]'] * df['Tool wear [min]']\n",
    "df['Heat dissipation [rpminK]'] = abs(df['Air temperature [K]'] - df['Process temperature [K]']) * df['Rotational speed [rpm]']\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "df[['Type', 'Failure Type']] = encoder.fit_transform(df[['Type', 'Failure Type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Type', 'Air temperature [K]', 'Process temperature [K]',\n",
      "       'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]', 'Target',\n",
      "       'Failure Type', 'Power [W]', 'Overstrain [minNm]',\n",
      "       'Heat dissipation [rpminK]'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RobustScaler on Rotational Speed and Torque is necessary because of strong outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "      <th>Power [W]</th>\n",
       "      <th>Overstrain [minNm]</th>\n",
       "      <th>Heat dissipation [rpminK]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Power [W]</th>\n",
       "      <th>Overstrain [minNm]</th>\n",
       "      <th>Heat dissipation [rpminK]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6951.590560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16285.5</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.474003</td>\n",
       "      <td>-0.929070</td>\n",
       "      <td>0.381501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6826.722724</td>\n",
       "      <td>138.9</td>\n",
       "      <td>14784.0</td>\n",
       "      <td>-0.502646</td>\n",
       "      <td>0.459259</td>\n",
       "      <td>0.387271</td>\n",
       "      <td>-0.896863</td>\n",
       "      <td>-0.122967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7749.387543</td>\n",
       "      <td>247.0</td>\n",
       "      <td>15579.2</td>\n",
       "      <td>-0.026455</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>1.028150</td>\n",
       "      <td>-0.871797</td>\n",
       "      <td>0.144201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5927.504659</td>\n",
       "      <td>276.5</td>\n",
       "      <td>14903.2</td>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-0.044444</td>\n",
       "      <td>-0.237322</td>\n",
       "      <td>-0.864957</td>\n",
       "      <td>-0.082919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5897.816608</td>\n",
       "      <td>360.0</td>\n",
       "      <td>14784.0</td>\n",
       "      <td>-0.502646</td>\n",
       "      <td>-0.007407</td>\n",
       "      <td>-0.257943</td>\n",
       "      <td>-0.845596</td>\n",
       "      <td>-0.122967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0   2.0                298.1                    308.6                  1551.0   \n",
       "1   1.0                298.2                    308.7                  1408.0   \n",
       "2   1.0                298.1                    308.5                  1498.0   \n",
       "3   1.0                298.2                    308.6                  1433.0   \n",
       "4   1.0                298.2                    308.7                  1408.0   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min]  Target  Failure Type    Power [W]  \\\n",
       "0         42.8              0.0     0.0           1.0  6951.590560   \n",
       "1         46.3              3.0     0.0           1.0  6826.722724   \n",
       "2         49.4              5.0     0.0           1.0  7749.387543   \n",
       "3         39.5              7.0     0.0           1.0  5927.504659   \n",
       "4         40.0              9.0     0.0           1.0  5897.816608   \n",
       "\n",
       "   Overstrain [minNm]  Heat dissipation [rpminK]  Rotational speed [rpm]  \\\n",
       "0                 0.0                    16285.5                0.253968   \n",
       "1               138.9                    14784.0               -0.502646   \n",
       "2               247.0                    15579.2               -0.026455   \n",
       "3               276.5                    14903.2               -0.370370   \n",
       "4               360.0                    14784.0               -0.502646   \n",
       "\n",
       "   Torque [Nm]  Power [W]  Overstrain [minNm]  Heat dissipation [rpminK]  \n",
       "0     0.200000   0.474003           -0.929070                   0.381501  \n",
       "1     0.459259   0.387271           -0.896863                  -0.122967  \n",
       "2     0.688889   1.028150           -0.871797                   0.144201  \n",
       "3    -0.044444  -0.237322           -0.864957                  -0.082919  \n",
       "4    -0.007407  -0.257943           -0.845596                  -0.122967  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Define the columns to be scaled\n",
    "columns = ['Rotational speed [rpm]', 'Torque [Nm]', 'Power [W]', 'Overstrain [minNm]', 'Heat dissipation [rpminK]']\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the specified columns in the dataframe\n",
    "features_scaled = scaler.fit_transform(df[columns])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=columns)\n",
    "\n",
    "# Drop the original columns from the dataframe\n",
    "df_scaled.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the scaled features back to the dataframe\n",
    "df_scaled = pd.concat([df, features_scaled], axis=1)\n",
    "\n",
    "# Display the first 5 rows of the scaled dataframe\n",
    "df_scaled.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air temperature, Process temperature and tool wear get scaled over MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "      <th>Power [W]</th>\n",
       "      <th>Overstrain [minNm]</th>\n",
       "      <th>Heat dissipation [rpminK]</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6951.590560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16285.5</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6826.722724</td>\n",
       "      <td>138.9</td>\n",
       "      <td>14784.0</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.011858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7749.387543</td>\n",
       "      <td>247.0</td>\n",
       "      <td>15579.2</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.019763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5927.504659</td>\n",
       "      <td>276.5</td>\n",
       "      <td>14903.2</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.027668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5897.816608</td>\n",
       "      <td>360.0</td>\n",
       "      <td>14784.0</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.035573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Rotational speed [rpm]  Torque [Nm]  Target  Failure Type  \\\n",
       "0   2.0                  1551.0         42.8     0.0           1.0   \n",
       "1   1.0                  1408.0         46.3     0.0           1.0   \n",
       "2   1.0                  1498.0         49.4     0.0           1.0   \n",
       "3   1.0                  1433.0         39.5     0.0           1.0   \n",
       "4   1.0                  1408.0         40.0     0.0           1.0   \n",
       "\n",
       "     Power [W]  Overstrain [minNm]  Heat dissipation [rpminK]  \\\n",
       "0  6951.590560                 0.0                    16285.5   \n",
       "1  6826.722724               138.9                    14784.0   \n",
       "2  7749.387543               247.0                    15579.2   \n",
       "3  5927.504659               276.5                    14903.2   \n",
       "4  5897.816608               360.0                    14784.0   \n",
       "\n",
       "   Air temperature [K]  Process temperature [K]  Tool wear [min]  \n",
       "0             0.304348                 0.358025         0.000000  \n",
       "1             0.315217                 0.370370         0.011858  \n",
       "2             0.304348                 0.345679         0.019763  \n",
       "3             0.315217                 0.358025         0.027668  \n",
       "4             0.315217                 0.370370         0.035573  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns to be scaled\n",
    "columns = ['Air temperature [K]', 'Process temperature [K]', 'Tool wear [min]']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the specified columns in the dataframe\n",
    "features_scaled = scaler.fit_transform(df[columns])\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=columns)\n",
    "\n",
    "# Drop the original columns from the dataframe\n",
    "df.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the scaled features back to the dataframe\n",
    "df_scaled = pd.concat([df, features_scaled], axis=1)\n",
    "\n",
    "# Display the first few rows of the scaled dataframe\n",
    "df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that the values for training data and test data are well divided, as there is a small number of errors, especially in the existing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the stratified split...\n",
      "Target proportion in original dataset:\n",
      "Target\n",
      "0    0.966911\n",
      "1    0.033089\n",
      "Name: proportion, dtype: float64\n",
      "Target proportion in y_train dataset:\n",
      "Target\n",
      "0    0.966974\n",
      "1    0.033026\n",
      "Name: proportion, dtype: float64\n",
      "Target proportion in y_test dataset:\n",
      "Target\n",
      "0    0.96672\n",
      "1    0.03328\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop(['Target', 'Failure Type'], axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Reset indices\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Stratified shuffle split\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create train-test splits\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "# Check target proportions\n",
    "print('Checking the stratified split...')\n",
    "print('Target proportion in original dataset:')\n",
    "print(df['Target'].value_counts(normalize=True))\n",
    "\n",
    "print('Target proportion in y_train dataset:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print('Target proportion in y_test dataset:')\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_Train and Y_Test have an equally good distribution and a small difference in the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Testing\n",
    "\n",
    "Specifically, we aim to classify whether a machine is functioning correctly or if it is experiencing a fault. This initial step of binary classification - distinguishing between \"faulty\" and \"operational\" states - serves several crucial purposes:\n",
    "\n",
    "- Simplicity and Clarity\n",
    "- Early Fault Detection\n",
    "- Resource Allocation\n",
    "\n",
    "Given our binary classification problem, we want to test a variety of machine learning models to determine which one performs best on our dataset. The models we plan to test include:\n",
    "\n",
    "- Logistic Regression: A simple yet effective linear model for binary classification.\n",
    "- Decision Tree Classifier: Easy to interpret and visualize, capturing non-linear relationships.\n",
    "- Random Forest Classifier: An ensemble method that builds multiple decision trees to improve accuracy and reduce overfitting.\n",
    "- Balanced Random Forest Classifier:\n",
    "- Gradient Boosting Classifier: Sequentially builds trees, each one correcting the errors of the previous one. We will also test variants like XGBoost, LightGBM, and CatBoost.\n",
    "- Bagging Classifier:\n",
    "- Balanced Bagging Classifier:\n",
    "- Easy Ensemble Classifier:\n",
    "- Support Vector Machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression\n",
      "Fitting Random Forest\n",
      "Fitting Bagging Classifier\n",
      "Fitting Balanced Random Forest\n",
      "Fitting Balanced Bagging\n",
      "Fitting Easy Ensemble\n",
      "Fitting SVC\n",
      "Fitting Gradient Boosting\n",
      "Fitting Decision Tree Classifier\n",
      "                              f1  precision  recall  roc_auc\n",
      "Random Forest             0.9315     0.9575  0.9084   0.9748\n",
      "Bagging Classifier        0.9308     0.9621  0.9038   0.9450\n",
      "Gradient Boosting         0.9230     0.9356  0.9113   0.9815\n",
      "Decision Tree Classifier  0.9098     0.9099  0.9102   0.9102\n",
      "Balanced Bagging          0.7745     0.7076  0.9335   0.9827\n",
      "Balanced Random Forest    0.7230     0.6616  0.9274   0.9843\n",
      "Easy Ensemble             0.6928     0.6389  0.9077   0.9702\n",
      "Logistic Regression       0.5381     0.8111  0.5248   0.8320\n",
      "SVC                       0.5129     0.9837  0.5108   0.9235\n",
      "Confusion Matrix for Random Forest:\n",
      "[[2405    6]\n",
      " [  16   67]]\n",
      "\n",
      "Confusion Matrix for Bagging Classifier:\n",
      "[[2405    6]\n",
      " [  19   64]]\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      "[[2400   11]\n",
      " [  15   68]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'Bagging Classifier': BaggingClassifier(random_state=42),\n",
    "    'Balanced Random Forest': BalancedRandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'Balanced Bagging': BalancedBaggingClassifier(random_state=42, n_jobs=-1),\n",
    "    'Easy Ensemble': EasyEnsembleClassifier(random_state=42),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Fitting {name}')\n",
    "    \n",
    "    # Cross validation metrics test data\n",
    "    scoring = [\"f1_macro\", \"precision_macro\", \"recall_macro\", \"roc_auc\"]\n",
    "    cross_val_scores = cross_validate(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_test_cv = round(cross_val_scores[\"test_f1_macro\"].mean(), 4)\n",
    "    precision_test_cv = round(cross_val_scores[\"test_precision_macro\"].mean(), 4)\n",
    "    recall_test_cv = round(cross_val_scores[\"test_recall_macro\"].mean(), 4)\n",
    "    roc_auc_test_cv = round(cross_val_scores[\"test_roc_auc\"].mean(), 4)\n",
    "    \n",
    "    # Summary table\n",
    "    score_df = pd.DataFrame({\n",
    "                     'f1': f1_test_cv,\n",
    "                     'precision': precision_test_cv,\n",
    "                     'recall': recall_test_cv,\n",
    "                     'roc_auc': roc_auc_test_cv},\n",
    "                     index=[name])\n",
    "\n",
    "    results_df = pd.concat([results_df, score_df])\n",
    "\n",
    "results_df = results_df.sort_values(by='f1', ascending=False)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Get the names of the top 3 classifiers\n",
    "top_3_classifiers = results_df.head(3).index\n",
    "\n",
    "# Train and display confusion matrices for the top 3 classifiers\n",
    "for name in top_3_classifiers:\n",
    "    model = models[name]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f'Confusion Matrix for {name}:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three classifiers based on F1 score are:\n",
    "\n",
    "1. Bagging Classifier: F1 score of 0.8626 with a confusion matrix showing high precision and recall.\n",
    "2. Random Forest: F1 score of 0.8449, slightly lower than Bagging, but with excellent precision and roc_auc.\n",
    "3. Decision Tree Classifier: F1 score of 0.8437, closely following Random Forest, with balanced precision and recall.\n",
    "4. Gradient Boosting: F1 score of 0.8405, slightly lower than Decision Tree, but with high precision and roc_auc.\n",
    "\n",
    "These models demonstrate strong performance, particularly in precision and roc_auc, indicating effective classification capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary on Sampling and Hyperparameter Optimization\n",
    "\n",
    "In machine learning, especially when dealing with imbalanced datasets, it is crucial to apply sampling techniques to ensure that models are trained effectively. Alongside sampling, hyperparameter optimization is essential to fine-tune model performance and achieve the best results.\n",
    "\n",
    "Sampling involves adjusting the dataset to balance the class distribution. This can be done through:\n",
    "\n",
    "- Oversampling: Increasing the number of instances in the minority class.\n",
    "- Undersampling: Reducing the number of instances in the majority class.\n",
    "\n",
    "Hyperparameter Optimization involves searching for the optimal set of parameters for a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  RandomForestClassifier(criterion='entropy', random_state=42)\n",
      "Fitting:  BaggingClassifier(n_jobs=-1, random_state=42)\n",
      "Fitting:  DecisionTreeClassifier(random_state=42)\n",
      "Fitting:  GradientBoostingClassifier(random_state=42)\n",
      "                        model      f1     auc  precision  recall  \\\n",
      "0      RandomForestClassifier  0.9281  0.9082     0.9502  0.9082   \n",
      "0           BaggingClassifier  0.9234  0.8963     0.9548  0.8963   \n",
      "0  GradientBoostingClassifier  0.9225  0.9078     0.9385  0.9078   \n",
      "0      RandomForestClassifier  0.9144  0.9071     0.9219  0.9071   \n",
      "0  GradientBoostingClassifier  0.9043  0.8949     0.9142  0.8949   \n",
      "0           BaggingClassifier  0.9040  0.9063     0.9016  0.9063   \n",
      "0           BaggingClassifier  0.9012  0.9175     0.8862  0.9175   \n",
      "0      RandomForestClassifier  0.8976  0.9115     0.8847  0.9115   \n",
      "0      DecisionTreeClassifier  0.8954  0.8885     0.9025  0.8885   \n",
      "0  GradientBoostingClassifier  0.8928  0.9111     0.8762  0.9111   \n",
      "0  GradientBoostingClassifier  0.8832  0.8988     0.8689  0.8988   \n",
      "0  GradientBoostingClassifier  0.8790  0.9099     0.8527  0.9099   \n",
      "0      DecisionTreeClassifier  0.8777  0.9040     0.8548  0.9040   \n",
      "0  GradientBoostingClassifier  0.8767  0.9096     0.8491  0.9096   \n",
      "0      RandomForestClassifier  0.8737  0.9151     0.8404  0.9151   \n",
      "0      RandomForestClassifier  0.8737  0.9151     0.8404  0.9151   \n",
      "0           BaggingClassifier  0.8687  0.9202     0.8293  0.9202   \n",
      "0           BaggingClassifier  0.8552  0.9132     0.8126  0.9132   \n",
      "0      DecisionTreeClassifier  0.8544  0.8565     0.8524  0.8565   \n",
      "0      DecisionTreeClassifier  0.8527  0.9186     0.8062  0.9186   \n",
      "0      DecisionTreeClassifier  0.8395  0.9227     0.7857  0.9227   \n",
      "0      DecisionTreeClassifier  0.7462  0.9030     0.6842  0.9030   \n",
      "0           BaggingClassifier  0.7320  0.9001     0.6714  0.9001   \n",
      "0      RandomForestClassifier  0.7216  0.8978     0.6625  0.8978   \n",
      "0      RandomForestClassifier  0.3556  0.7105     0.5278  0.7105   \n",
      "0           BaggingClassifier  0.2760  0.6447     0.5225  0.6447   \n",
      "0  GradientBoostingClassifier  0.2659  0.6427     0.5230  0.6427   \n",
      "0      DecisionTreeClassifier  0.2658  0.6202     0.5192  0.6202   \n",
      "\n",
      "     sampling_method  n_estimators  min_samples_split  max_depth  \\\n",
      "0         TomekLinks          60.0                5.0       30.0   \n",
      "0         TomekLinks         100.0                NaN        NaN   \n",
      "0  RandomOverSampler         140.0               10.0        9.0   \n",
      "0  RandomOverSampler          80.0                5.0       30.0   \n",
      "0         TomekLinks         290.0               10.0        5.0   \n",
      "0  RandomOverSampler          10.0                NaN        NaN   \n",
      "0    BorderlineSMOTE         120.0                NaN        NaN   \n",
      "0    BorderlineSMOTE          80.0                5.0       30.0   \n",
      "0         TomekLinks           NaN                5.0       36.0   \n",
      "0    BorderlineSMOTE         140.0               10.0        9.0   \n",
      "0           NearMiss          10.0                2.0        5.0   \n",
      "0         SMOTETomek         140.0               10.0        9.0   \n",
      "0  RandomOverSampler           NaN                5.0       31.0   \n",
      "0              SMOTE         290.0               10.0        5.0   \n",
      "0              SMOTE         170.0                2.0       90.0   \n",
      "0         SMOTETomek         170.0                2.0       90.0   \n",
      "0              SMOTE          50.0                NaN        NaN   \n",
      "0         SMOTETomek         150.0                NaN        NaN   \n",
      "0    BorderlineSMOTE           NaN                5.0       36.0   \n",
      "0              SMOTE           NaN                5.0       31.0   \n",
      "0         SMOTETomek           NaN                5.0       31.0   \n",
      "0           NearMiss           NaN                2.0       21.0   \n",
      "0           NearMiss         120.0                NaN        NaN   \n",
      "0           NearMiss          80.0                5.0       30.0   \n",
      "0   ClusterCentroids         170.0                2.0       90.0   \n",
      "0   ClusterCentroids         120.0                NaN        NaN   \n",
      "0   ClusterCentroids         230.0                5.0        5.0   \n",
      "0   ClusterCentroids           NaN                5.0        6.0   \n",
      "\n",
      "   min_samples_leaf  learning_rate  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            0.1  \n",
      "0               NaN            1.0  \n",
      "0               1.0            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               2.0            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               1.0            NaN  \n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "import joblib\n",
    "\n",
    "# Define models\n",
    "RdFo = RandomForestClassifier(random_state=42, criterion='entropy')\n",
    "BBC = BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "DTC = DecisionTreeClassifier(random_state=42)\n",
    "GBC = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create NearestNeighbors object with n_jobs set\n",
    "nn = NearestNeighbors(n_jobs=-1)\n",
    "\n",
    "# Define sampling methods with NearestNeighbors object\n",
    "OverSamp_1 = RandomOverSampler(random_state=42)\n",
    "OverSamp_2 = SMOTE(random_state=42, k_neighbors=nn)\n",
    "OverSamp_3 = BorderlineSMOTE(random_state=42, k_neighbors=nn)\n",
    "UnderSamp_1 = ClusterCentroids(random_state=42)\n",
    "UnderSamp_2 = TomekLinks(n_jobs=-1)\n",
    "UnderSamp_3 = NearMiss(version=3, n_jobs=-1)\n",
    "Samp_7 = SMOTETomek()\n",
    "\n",
    "# Combine over- and undersampling methods into a list\n",
    "Samp_list = [OverSamp_1, OverSamp_2, OverSamp_3, UnderSamp_1, UnderSamp_2, UnderSamp_3, Samp_7]\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "#Placeholder for models\n",
    "best_models = []\n",
    "\n",
    "# Loop through each model and each sampling method\n",
    "for model in [RdFo, BBC, DTC, GBC]:\n",
    "    print(\"Fitting: \", model)\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 300, 10),\n",
    "            'max_depth': np.arange(10, 100, 10),\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    elif isinstance(model, BaggingClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 160, 10)\n",
    "        }\n",
    "    elif isinstance(model, DecisionTreeClassifier):\n",
    "        grid_param = {\n",
    "            'max_depth': np.arange(1, 50, 5),\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif isinstance(model, GradientBoostingClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 300, 10),\n",
    "            'learning_rate': np.logspace(-3, 0, 4),\n",
    "            'max_depth': np.arange(1, 10, 2),\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "                      \n",
    "    for samp in Samp_list:\n",
    "        # Resample the training data\n",
    "        X_train_resampled, y_train_resampled = samp.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Perform Randomized Search with cross-validation\n",
    "        random_search = RandomizedSearchCV(model, grid_param, cv=3, n_jobs=-1, scoring='f1_macro', refit='f1_macro', random_state=42)\n",
    "        random_search.fit(X_train_resampled, y_train_resampled)\n",
    "        y_pred = random_search.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "        \n",
    "        # Create DataFrame for results\n",
    "        score_df = pd.DataFrame({\n",
    "            'model': [str(model).split('(')[0]],\n",
    "            'f1': [f1],\n",
    "            'auc': [auc],\n",
    "            'precision': [precision],\n",
    "            'recall': [recall],\n",
    "            'sampling_method': [str(samp).split('(')[0]]\n",
    "        })\n",
    "        \n",
    "        # Append best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        for param in best_params:\n",
    "            \n",
    "            score_df[param] = best_params[param]\n",
    "        \n",
    "        results_df = pd.concat([results_df, score_df])\n",
    "\n",
    "        if len(best_models) < 2:\n",
    "            best_models.append((random_search.best_estimator_, samp, f1))\n",
    "        else:\n",
    "            min_f1 = min(best_models, key=lambda x: x[2])\n",
    "            if f1 > min_f1[2]:\n",
    "                best_models.remove(min_f1)\n",
    "                best_models.append((random_search.best_estimator_, samp, f1))\n",
    "\n",
    "\n",
    "# Sort results by f1 score and display\n",
    "results_df = results_df.sort_values(by='f1', ascending=False)\n",
    "print(results_df.round(4))\n",
    "\n",
    "for i, (model, samp, f1) in enumerate(best_models):\n",
    "    joblib.dump(model, f\"best_model_{i+1}.joblib\")\n",
    "    joblib.dump(samp, f\"best_sampling_{i+1}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results were achieved with various models and the sampling method TomekLinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting:  RandomForestClassifier(criterion='entropy', random_state=42)\n",
      "Fitting:  BaggingClassifier(n_jobs=-1, random_state=42)\n",
      "Fitting:  DecisionTreeClassifier(random_state=42)\n",
      "Fitting:  GradientBoostingClassifier(random_state=42)\n",
      "                        model      f1     auc  precision  recall  \\\n",
      "0      RandomForestClassifier  0.9281  0.9082     0.9502  0.9082   \n",
      "0           BaggingClassifier  0.9234  0.8963     0.9548  0.8963   \n",
      "0  GradientBoostingClassifier  0.9225  0.9078     0.9385  0.9078   \n",
      "0      RandomForestClassifier  0.9144  0.9071     0.9219  0.9071   \n",
      "0  GradientBoostingClassifier  0.9043  0.8949     0.9142  0.8949   \n",
      "0           BaggingClassifier  0.9040  0.9063     0.9016  0.9063   \n",
      "0           BaggingClassifier  0.9012  0.9175     0.8862  0.9175   \n",
      "0      RandomForestClassifier  0.8976  0.9115     0.8847  0.9115   \n",
      "0      DecisionTreeClassifier  0.8954  0.8885     0.9025  0.8885   \n",
      "0  GradientBoostingClassifier  0.8928  0.9111     0.8762  0.9111   \n",
      "0  GradientBoostingClassifier  0.8832  0.8988     0.8689  0.8988   \n",
      "0      DecisionTreeClassifier  0.8777  0.9040     0.8548  0.9040   \n",
      "0  GradientBoostingClassifier  0.8767  0.9096     0.8491  0.9096   \n",
      "0      RandomForestClassifier  0.8737  0.9151     0.8404  0.9151   \n",
      "0           BaggingClassifier  0.8687  0.9202     0.8293  0.9202   \n",
      "0      RandomForestClassifier  0.8673  0.9144     0.8306  0.9144   \n",
      "0  GradientBoostingClassifier  0.8625  0.9026     0.8302  0.9026   \n",
      "0           BaggingClassifier  0.8612  0.9138     0.8213  0.9138   \n",
      "0      DecisionTreeClassifier  0.8544  0.8565     0.8524  0.8565   \n",
      "0      DecisionTreeClassifier  0.8527  0.9186     0.8062  0.9186   \n",
      "0      DecisionTreeClassifier  0.8374  0.9281     0.7808  0.9281   \n",
      "0      DecisionTreeClassifier  0.7462  0.9030     0.6842  0.9030   \n",
      "0           BaggingClassifier  0.7320  0.9001     0.6714  0.9001   \n",
      "0      RandomForestClassifier  0.7216  0.8978     0.6625  0.8978   \n",
      "0      RandomForestClassifier  0.3556  0.7105     0.5278  0.7105   \n",
      "0           BaggingClassifier  0.2760  0.6447     0.5225  0.6447   \n",
      "0  GradientBoostingClassifier  0.2659  0.6427     0.5230  0.6427   \n",
      "0      DecisionTreeClassifier  0.2658  0.6202     0.5192  0.6202   \n",
      "\n",
      "     sampling_method  n_estimators  min_samples_split  max_depth  \\\n",
      "0         TomekLinks          60.0                5.0       30.0   \n",
      "0         TomekLinks         100.0                NaN        NaN   \n",
      "0  RandomOverSampler         140.0               10.0        9.0   \n",
      "0  RandomOverSampler          80.0                5.0       30.0   \n",
      "0         TomekLinks         290.0               10.0        5.0   \n",
      "0  RandomOverSampler          10.0                NaN        NaN   \n",
      "0    BorderlineSMOTE         120.0                NaN        NaN   \n",
      "0    BorderlineSMOTE          80.0                5.0       30.0   \n",
      "0         TomekLinks           NaN                5.0       36.0   \n",
      "0    BorderlineSMOTE         140.0               10.0        9.0   \n",
      "0           NearMiss          10.0                2.0        5.0   \n",
      "0  RandomOverSampler           NaN                5.0       31.0   \n",
      "0              SMOTE         290.0               10.0        5.0   \n",
      "0              SMOTE         170.0                2.0       90.0   \n",
      "0              SMOTE          50.0                NaN        NaN   \n",
      "0         SMOTETomek          60.0                2.0       40.0   \n",
      "0         SMOTETomek         140.0               10.0        9.0   \n",
      "0         SMOTETomek         120.0                NaN        NaN   \n",
      "0    BorderlineSMOTE           NaN                5.0       36.0   \n",
      "0              SMOTE           NaN                5.0       31.0   \n",
      "0         SMOTETomek           NaN                5.0       31.0   \n",
      "0           NearMiss           NaN                2.0       21.0   \n",
      "0           NearMiss         120.0                NaN        NaN   \n",
      "0           NearMiss          80.0                5.0       30.0   \n",
      "0   ClusterCentroids         170.0                2.0       90.0   \n",
      "0   ClusterCentroids         120.0                NaN        NaN   \n",
      "0   ClusterCentroids         230.0                5.0        5.0   \n",
      "0   ClusterCentroids           NaN                5.0        6.0   \n",
      "\n",
      "   min_samples_leaf  learning_rate  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            0.1  \n",
      "0               1.0            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               2.0            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               1.0            NaN  \n",
      "Stacking Model - F1: 0.9321427016673196, AUC: 0.8969682129596778, Precision: 0.9747557149260422, Recall: 0.8969682129596778\n",
      "                        model      f1     auc  precision  recall  \\\n",
      "0      RandomForestClassifier  0.9281  0.9082     0.9502  0.9082   \n",
      "0           BaggingClassifier  0.9234  0.8963     0.9548  0.8963   \n",
      "0  GradientBoostingClassifier  0.9225  0.9078     0.9385  0.9078   \n",
      "0      RandomForestClassifier  0.9144  0.9071     0.9219  0.9071   \n",
      "0  GradientBoostingClassifier  0.9043  0.8949     0.9142  0.8949   \n",
      "0           BaggingClassifier  0.9040  0.9063     0.9016  0.9063   \n",
      "0           BaggingClassifier  0.9012  0.9175     0.8862  0.9175   \n",
      "0      RandomForestClassifier  0.8976  0.9115     0.8847  0.9115   \n",
      "0      DecisionTreeClassifier  0.8954  0.8885     0.9025  0.8885   \n",
      "0  GradientBoostingClassifier  0.8928  0.9111     0.8762  0.9111   \n",
      "0  GradientBoostingClassifier  0.8832  0.8988     0.8689  0.8988   \n",
      "0      DecisionTreeClassifier  0.8777  0.9040     0.8548  0.9040   \n",
      "0  GradientBoostingClassifier  0.8767  0.9096     0.8491  0.9096   \n",
      "0      RandomForestClassifier  0.8737  0.9151     0.8404  0.9151   \n",
      "0           BaggingClassifier  0.8687  0.9202     0.8293  0.9202   \n",
      "0      RandomForestClassifier  0.8673  0.9144     0.8306  0.9144   \n",
      "0  GradientBoostingClassifier  0.8625  0.9026     0.8302  0.9026   \n",
      "0           BaggingClassifier  0.8612  0.9138     0.8213  0.9138   \n",
      "0      DecisionTreeClassifier  0.8544  0.8565     0.8524  0.8565   \n",
      "0      DecisionTreeClassifier  0.8527  0.9186     0.8062  0.9186   \n",
      "0      DecisionTreeClassifier  0.8374  0.9281     0.7808  0.9281   \n",
      "0      DecisionTreeClassifier  0.7462  0.9030     0.6842  0.9030   \n",
      "0           BaggingClassifier  0.7320  0.9001     0.6714  0.9001   \n",
      "0      RandomForestClassifier  0.7216  0.8978     0.6625  0.8978   \n",
      "0      RandomForestClassifier  0.3556  0.7105     0.5278  0.7105   \n",
      "0           BaggingClassifier  0.2760  0.6447     0.5225  0.6447   \n",
      "0  GradientBoostingClassifier  0.2659  0.6427     0.5230  0.6427   \n",
      "0      DecisionTreeClassifier  0.2658  0.6202     0.5192  0.6202   \n",
      "0          StackingClassifier  0.9321  0.8970     0.9748  0.8970   \n",
      "\n",
      "     sampling_method  n_estimators  min_samples_split  max_depth  \\\n",
      "0         TomekLinks          60.0                5.0       30.0   \n",
      "0         TomekLinks         100.0                NaN        NaN   \n",
      "0  RandomOverSampler         140.0               10.0        9.0   \n",
      "0  RandomOverSampler          80.0                5.0       30.0   \n",
      "0         TomekLinks         290.0               10.0        5.0   \n",
      "0  RandomOverSampler          10.0                NaN        NaN   \n",
      "0    BorderlineSMOTE         120.0                NaN        NaN   \n",
      "0    BorderlineSMOTE          80.0                5.0       30.0   \n",
      "0         TomekLinks           NaN                5.0       36.0   \n",
      "0    BorderlineSMOTE         140.0               10.0        9.0   \n",
      "0           NearMiss          10.0                2.0        5.0   \n",
      "0  RandomOverSampler           NaN                5.0       31.0   \n",
      "0              SMOTE         290.0               10.0        5.0   \n",
      "0              SMOTE         170.0                2.0       90.0   \n",
      "0              SMOTE          50.0                NaN        NaN   \n",
      "0         SMOTETomek          60.0                2.0       40.0   \n",
      "0         SMOTETomek         140.0               10.0        9.0   \n",
      "0         SMOTETomek         120.0                NaN        NaN   \n",
      "0    BorderlineSMOTE           NaN                5.0       36.0   \n",
      "0              SMOTE           NaN                5.0       31.0   \n",
      "0         SMOTETomek           NaN                5.0       31.0   \n",
      "0           NearMiss           NaN                2.0       21.0   \n",
      "0           NearMiss         120.0                NaN        NaN   \n",
      "0           NearMiss          80.0                5.0       30.0   \n",
      "0   ClusterCentroids         170.0                2.0       90.0   \n",
      "0   ClusterCentroids         120.0                NaN        NaN   \n",
      "0   ClusterCentroids         230.0                5.0        5.0   \n",
      "0   ClusterCentroids           NaN                5.0        6.0   \n",
      "0                NaN           NaN                NaN        NaN   \n",
      "\n",
      "   min_samples_leaf  learning_rate  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            0.1  \n",
      "0               1.0            NaN  \n",
      "0               NaN            0.1  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            1.0  \n",
      "0               NaN            NaN  \n",
      "0               4.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               1.0            NaN  \n",
      "0               2.0            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            NaN  \n",
      "0               NaN            0.1  \n",
      "0               1.0            NaN  \n",
      "0               NaN            NaN  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define models\n",
    "RdFo = RandomForestClassifier(random_state=42, criterion='entropy')\n",
    "BBC = BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "DTC = DecisionTreeClassifier(random_state=42)\n",
    "GBC = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create NearestNeighbors object with n_jobs set\n",
    "nn = NearestNeighbors(n_jobs=-1)\n",
    "\n",
    "# Define sampling methods with NearestNeighbors object\n",
    "OverSamp_1 = RandomOverSampler(random_state=42)\n",
    "OverSamp_2 = SMOTE(random_state=42, k_neighbors=nn)\n",
    "OverSamp_3 = BorderlineSMOTE(random_state=42, k_neighbors=nn)\n",
    "UnderSamp_1 = ClusterCentroids(random_state=42)\n",
    "UnderSamp_2 = TomekLinks(n_jobs=-1)\n",
    "UnderSamp_3 = NearMiss(version=3, n_jobs=-1)\n",
    "Samp_7 = SMOTETomek()\n",
    "\n",
    "# Combine over- and undersampling methods into a list\n",
    "Samp_list = [OverSamp_1, OverSamp_2, OverSamp_3, UnderSamp_1, UnderSamp_2, UnderSamp_3, Samp_7]\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Placeholder for models\n",
    "best_models = []\n",
    "\n",
    "# Loop through each model and each sampling method\n",
    "for model in [RdFo, BBC, DTC, GBC]:\n",
    "    print(\"Fitting: \", model)\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 300, 10),\n",
    "            'max_depth': np.arange(10, 100, 10),\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    elif isinstance(model, BaggingClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 160, 10)\n",
    "        }\n",
    "    elif isinstance(model, DecisionTreeClassifier):\n",
    "        grid_param = {\n",
    "            'max_depth': np.arange(1, 50, 5),\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif isinstance(model, GradientBoostingClassifier):\n",
    "        grid_param = {\n",
    "            'n_estimators': np.arange(10, 300, 10),\n",
    "            'learning_rate': np.logspace(-3, 0, 4),\n",
    "            'max_depth': np.arange(1, 10, 2),\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "                      \n",
    "    for samp in Samp_list:\n",
    "        # Resample the training data\n",
    "        X_train_resampled, y_train_resampled = samp.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Perform Randomized Search with cross-validation\n",
    "        random_search = RandomizedSearchCV(model, grid_param, cv=3, n_jobs=-1, scoring='f1_macro', refit='f1_macro', random_state=42)\n",
    "        random_search.fit(X_train_resampled, y_train_resampled)\n",
    "        y_pred = random_search.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "        \n",
    "        # Create DataFrame for results\n",
    "        score_df = pd.DataFrame({\n",
    "            'model': [str(model).split('(')[0]],\n",
    "            'f1': [f1],\n",
    "            'auc': [auc],\n",
    "            'precision': [precision],\n",
    "            'recall': [recall],\n",
    "            'sampling_method': [str(samp).split('(')[0]]\n",
    "        })\n",
    "        \n",
    "        # Append best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        for param in best_params:\n",
    "            score_df[param] = best_params[param]\n",
    "        \n",
    "        results_df = pd.concat([results_df, score_df])\n",
    "\n",
    "# Sort results by f1 score and display\n",
    "results_df = results_df.sort_values(by='f1', ascending=False)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Define a Stacking Classifier with the best models\n",
    "estimators = [(f'best_model_{i+1}', best_models[i][0]) for i in range(len(best_models))]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Train the Stacking Classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Stacking Classifier\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "f1_stacking = f1_score(y_test, y_pred_stacking, average=\"macro\")\n",
    "auc_stacking = roc_auc_score(y_test, y_pred_stacking)\n",
    "precision_stacking = precision_score(y_test, y_pred_stacking, average=\"macro\")\n",
    "recall_stacking = recall_score(y_test, y_pred_stacking, average=\"macro\")\n",
    "\n",
    "print(f\"Stacking Model - F1: {f1_stacking}, AUC: {auc_stacking}, Precision: {precision_stacking}, Recall: {recall_stacking}\")\n",
    "\n",
    "# Save Stacking Classifier Performance\n",
    "stacking_performance = pd.DataFrame({\n",
    "    'model': ['StackingClassifier'],\n",
    "    'f1': [f1_stacking],\n",
    "    'auc': [auc_stacking],\n",
    "    'precision': [precision_stacking],\n",
    "    'recall': [recall_stacking]\n",
    "})\n",
    "\n",
    "results_df = pd.concat([results_df, stacking_performance])\n",
    "print(results_df.round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best models and their results demonstrate strong performance across various metrics. The RandomForestClassifier with the hyperparameters n_estimators=60, min_samples_split=5, and max_depth=30, combined with the sampling method TomekLinks, achieved the highest F1 score (0.9281) and AUC (0.9082). These results indicate that this model strikes a very good balance between precision and recall, delivering robust classification outcomes.\n",
    "\n",
    "The BaggingClassifier with n_estimators=100, also using TomekLinks, reached an F1 score of 0.9234 and an AUC of 0.8963. Despite a slightly lower AUC, the model shows a high precision of 0.9548, meaning it is very effective at correctly identifying the positive classes.\n",
    "\n",
    "The GradientBoostingClassifier with the hyperparameters n_estimators=140, min_samples_split=10, and max_depth=9, combined with the RandomOverSampler sampling method, achieved an F1 score of 0.9225 and an AUC of 0.9078. This combination shows that Gradient Boosting is also very effective, particularly in improving recall values through the RandomOverSampler method, leading to balanced and accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Library\n",
    "\n",
    "The Pickle library in Python serializes and deserializes Python objects, known as pickling and unpickling. This allows saving objects to a file and loading them later while preserving their state. It's commonly used for saving machine learning models, data preprocessing steps, and complex data structures between sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best models and sampling methods have been saved to .././best_models_sampling.pkl.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the best models and sampling methods\n",
    "best_model_1 = RandomForestClassifier(n_estimators=60, min_samples_split=5, max_depth=30, criterion='entropy', random_state=42)\n",
    "best_sampling_1 = TomekLinks(n_jobs=-1)\n",
    "\n",
    "best_model_2 = BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
    "best_sampling_2 = TomekLinks(n_jobs=-1)\n",
    "\n",
    "best_model_3 = GradientBoostingClassifier(n_estimators=140, min_samples_split=10, max_depth=9, random_state=42)\n",
    "best_sampling_3 = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Store the best models and sampling methods in a dictionary\n",
    "best_models_sampling = {\n",
    "    \"best_model_1\": best_model_1,\n",
    "    \"best_sampling_1\": best_sampling_1,\n",
    "    \"best_model_2\": best_model_2,\n",
    "    \"best_sampling_2\": best_sampling_2,\n",
    "    \"best_model_3\": best_model_3,\n",
    "    \"best_sampling_3\": best_sampling_3\n",
    "}\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "path_to_pickle_file = \".././best_models_sampling.pkl\"\n",
    "with open(path_to_pickle_file, \"wb\") as file:\n",
    "    pickle.dump(best_models_sampling, file)\n",
    "\n",
    "print(f\"The best models and sampling methods have been saved to {path_to_pickle_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
